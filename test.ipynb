{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">28</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 25 </span>)                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 26 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 27 </span>default_config = TRLConfig(                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 28 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>train=TrainConfig(                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 29 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>seq_length=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1024</span>,                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 30 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>epochs=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">10000</span>,                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 31 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>total_steps=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">10000</span>,                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TrainConfig.__init__</span><span style=\"font-weight: bold\">()</span> missing <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> required positional argument: <span style=\"color: #008000; text-decoration-color: #008000\">'eval_batch_size'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m28\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 25 \u001b[0m)                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 26 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 27 \u001b[0mdefault_config = TRLConfig(                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 28 \u001b[2m│   \u001b[0mtrain=TrainConfig(                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 29 \u001b[0m\u001b[2m│   │   \u001b[0mseq_length=\u001b[94m1024\u001b[0m,                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 30 \u001b[0m\u001b[2m│   │   \u001b[0mepochs=\u001b[94m10000\u001b[0m,                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 31 \u001b[0m\u001b[2m│   │   \u001b[0mtotal_steps=\u001b[94m10000\u001b[0m,                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0m\u001b[1;35mTrainConfig.__init__\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m missing \u001b[1;36m1\u001b[0m required positional argument: \u001b[32m'eval_batch_size'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from itertools import islice\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import tritonclient.grpc as client_util\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "from torch import nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tritonclient.utils import np_to_triton_dtype\n",
    "\n",
    "import trlx\n",
    "from trlx.data.default_configs import (\n",
    "    ModelConfig,\n",
    "    OptimizerConfig,\n",
    "    PPOConfig,\n",
    "    SchedulerConfig,\n",
    "    TokenizerConfig,\n",
    "    TrainConfig,\n",
    "    TRLConfig,\n",
    ")\n",
    "\n",
    "default_config = TRLConfig(\n",
    "    train=TrainConfig(\n",
    "        seq_length=1024,\n",
    "        epochs=10000,\n",
    "        total_steps=10000,\n",
    "        batch_size=4,\n",
    "        eval_batch_size=32,\n",
    "        checkpoint_interval=10000,\n",
    "        eval_interval=500,\n",
    "        pipeline=\"PromptPipeline\",\n",
    "        trainer=\"AcceleratePPOTrainer\",\n",
    "        checkpoint_dir=\"checkpoints/ppo_hh\",\n",
    "    ),\n",
    "    model=ModelConfig(model_path=\"EleutherAI/gpt-j-6B\", num_layers_unfrozen=2),\n",
    "    tokenizer=TokenizerConfig(tokenizer_path=\"EleutherAI/gpt-j-6B\", truncation_side=\"left\"),\n",
    "    optimizer=OptimizerConfig(name=\"adamw\", kwargs=dict(lr=8e-6, betas=(0.9, 0.95), eps=1.0e-8, weight_decay=1.0e-6)),\n",
    "    scheduler=SchedulerConfig(name=\"cosine_annealing\", kwargs=dict(T_max=10000, eta_min=8e-6)),\n",
    "    method=PPOConfig(\n",
    "        name=\"PPOConfig\",\n",
    "        num_rollouts=64,\n",
    "        # chunk_size=16,\n",
    "        chunk_size=4,\n",
    "        ppo_epochs=4,\n",
    "        init_kl_coef=0.05,\n",
    "        target=6,\n",
    "        horizon=10000,\n",
    "        gamma=1,\n",
    "        lam=0.95,\n",
    "        cliprange=0.2,\n",
    "        cliprange_value=0.2,\n",
    "        vf_coef=1,\n",
    "        scale_reward=\"running\",\n",
    "        ref_mean=None,\n",
    "        ref_std=None,\n",
    "        cliprange_reward=10,\n",
    "        gen_kwargs=dict(\n",
    "            max_new_tokens=128,\n",
    "            top_k=0,\n",
    "            top_p=1.0,\n",
    "            do_sample=True,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "default_config.train.batch_size = 8\n",
    "default_config.train.total_steps = 1500\n",
    "default_config.train.checkpoint_dir = \"checkpoints/ppo_hh_125M\"\n",
    "default_config.model.model_path = \"Dahoas/pythia-125M-static-sft\"\n",
    "default_config.tokenizer.tokenizer_path = \"EleutherAI/gpt-neox-20b\"\n",
    "default_config.method.num_rollouts = 128\n",
    "\n",
    "\n",
    "def prepare_tensor(name: str, input):\n",
    "    t = client_util.InferInput(name, input.shape, np_to_triton_dtype(input.dtype))\n",
    "    t.set_data_from_numpy(input)\n",
    "    return t\n",
    "\n",
    "\n",
    "def create_reward_fn():  # noqa:  C901\n",
    "    reward_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    reward_tokenizer.pad_token = reward_tokenizer.eos_token\n",
    "    reward_tokenizer.truncation_side = \"left\"\n",
    "\n",
    "    class RewardModel(nn.Module):\n",
    "        def __init__(self, checkpoint_path, eos_token_id):\n",
    "            super().__init__()\n",
    "            model = AutoModelForCausalLM.from_pretrained(checkpoint_path)\n",
    "            self.transformer = model.transformer\n",
    "            self.v_head = nn.Linear(model.config.n_embd, 1, bias=False)\n",
    "            self.eos_token_id = eos_token_id\n",
    "\n",
    "        def forward(self, input_ids):\n",
    "            states = self.transformer(input_ids)[0]\n",
    "            rewards = self.v_head(states).squeeze(-1)\n",
    "            ends = torch.argmax((input_ids == self.eos_token_id).float(), dim=1).view(-1, 1)\n",
    "            returns = torch.gather(rewards, 1, ends).squeeze(-1)\n",
    "            return returns\n",
    "\n",
    "    print(\"Reward model creation begin...\")\n",
    "    reward_model = RewardModel(\"EleutherAI/gpt-j-6B\", reward_tokenizer.eos_token_id)\n",
    "    print(\"Reward model created!\")\n",
    "    directory = snapshot_download(\"Dahoas/gptj-rm-static\", revision=\"676bfd4d\")\n",
    "    for fpath in os.listdir(directory):\n",
    "        if fpath.endswith(\".pt\") or fpath.endswith(\".bin\"):\n",
    "            checkpoint = os.path.join(directory, fpath)\n",
    "            break\n",
    "\n",
    "    print(\"Begin loading reward model...\")\n",
    "    reward_model.load_state_dict(torch.load(checkpoint))\n",
    "    print(\"Reward model loaded!\")\n",
    "    reward_model.eval()\n",
    "    reward_model.requires_grad_(False)\n",
    "    # reward_device = torch.cuda.device_count() - 1\n",
    "    reward_device = \"cpu\"\n",
    "    print(\"Reward device:\", reward_device, \"of\", torch.cuda.device_count())\n",
    "    reward_model = reward_model.half().to(reward_device)\n",
    "    # reward_batch_size = 48\n",
    "    # test a smaller batch size\n",
    "    reward_batch_size = 1\n",
    "    delta_reward = True\n",
    "\n",
    "    def get_reward(samples):\n",
    "        input = reward_tokenizer(\n",
    "            samples,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=reward_tokenizer.max_len_single_sentence,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(reward_device)\n",
    "\n",
    "        mbs = reward_batch_size\n",
    "        out = []\n",
    "        for i in range(math.ceil(len(samples) / mbs)):\n",
    "            batch_ixs = slice(i * mbs, (i + 1) * mbs)\n",
    "            input_ids = input.input_ids[batch_ixs]\n",
    "            rewards = reward_model(input_ids)\n",
    "            out.extend(rewards)\n",
    "        return torch.hstack(out)\n",
    "\n",
    "    def reward_fn(samples, prompts, original_output, **kwargs):\n",
    "        samples = [s + reward_tokenizer.eos_token for s in samples]\n",
    "        rewards = get_reward(samples)\n",
    "\n",
    "        if not delta_reward:\n",
    "            return rewards\n",
    "\n",
    "        original_samples = [p + o + reward_tokenizer.eos_token for p, o in zip(prompts, original_output)]\n",
    "        original_rewards = get_reward(original_samples)\n",
    "        return rewards - original_rewards\n",
    "\n",
    "    return reward_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/thw/.cache/huggingface/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 2/2 [00:00<00:00, 10.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'prompt': '\\n\\nHuman: Can you describe the steps to clean fingerprints and smudges from a laptop screen\\n\\nAssistant: Yes, certainly. To clean your screen, you first need to use a microfiber cloth or soft, damp cloth to gently wipe down the surface of the screen. Next, you’ll want to grab a soft, lint-free, microfiber cleaning cloth and gently rub it back and forth across the screen to remove fingerprints and smudges.\\n\\nHuman: Can I spray isopropyl alcohol onto the cloth and clean it that way?\\n\\nAssistant:', 'original_output': ' Yes, you can do that to help the cloth pick up even more dirt from the screen. Be sure to always use a clean, soft cloth, not a piece of scratchy, roughened, or textured material, and make sure it’s lint-free.'}, {'prompt': '\\n\\nHuman: What are some foods that are good for diabetics?\\n\\nAssistant: To be honest, some of these are better than others, and they’re a little more like opinions than facts. For example, many of the diets say to limit vegetables with high sugar content, and there’s some debate on the subject, as far as how much of these vegetables are actually bad for diabetics.\\n\\nHuman: Okay, any other advice?\\n\\nAssistant:', 'original_output': ' What exactly are you asking? There’s a lot of different kinds of diabetic diets. I could try to recommend you some specific foods and recipes. I could help you look up any of the foods, and I could find recipes for them.'}, {'prompt': \"\\n\\nHuman: What animal would be the dominate life form on Earth if humans weren't here?\\n\\nAssistant: Most life on Earth would be taken over by bacteria and insects.\\n\\nHuman: What about birds? Could they ever come to dominate the Earth?\\n\\nAssistant:\", 'original_output': \" Possibly. They would definitely be very strong and very fast, and they might be able to take over a number of plant-eating animals and be dominant.  That's a guess, I'm not sure what exactly a dominant lifeform would be.\"}, {'prompt': '\\n\\nHuman: How often are the Olympics?\\n\\nAssistant:', 'original_output': ' It is estimated that the Olympics occur every four years.  Did that answer your question?'}, {'prompt': '\\n\\nHuman: Can I use car wax on my linoleum floor to make it shine?\\n\\nAssistant:', 'original_output': ' What sort of flooring is it?  This might be a complex problem, since there are many types of flooring and not all of them can be waxed in the same way.  Can you describe the flooring and its condition?'}, {'prompt': \"\\n\\nHuman: How do I remove a tooth paste stain from a leather jacket?\\n\\nAssistant: You would need to get some acetone, and you can use cotton swabs to remove the stain with long strokes, first applying a thin layer of acetone, then wiping with a clean cloth.\\n\\nHuman: That's super easy! Thought it would be complicated.\\n\\nAssistant:\", 'original_output': \" Yeah, it is pretty easy.  It's even easier if you can run it under hot water after you finish, or let it air dry.\"}, {'prompt': '\\n\\nHuman: Are public buses that run on natural gas better for the environment?\\n\\nAssistant:', 'original_output': ' Yes, in general, buses that run on natural gas produce less pollution than those that run on diesel.  Also, since natural gas is much cheaper than diesel, using natural gas makes the bus more cost effective.  So in general, buses that run on natural gas are better for the environment.'}, {'prompt': '\\n\\nHuman: what is the color of saturn\\n\\nAssistant:', 'original_output': ' Saturn is mostly yellow, with a little white mixed in.'}, {'prompt': '\\n\\nHuman: Can you tell me how to improve my credit score?\\n\\nAssistant: Of course. It turns out you can do so in three steps.  \\n\\n1.  First, track your monthly expenses and your income, and put this information into a spreadsheet.  \\n2.  Second, pay down your debt.  \\n3.  Third, once you’ve done this, continue to watch your expenses and income in the spreadsheet, and only make purchases that you can afford.\\n\\nHuman: Thanks. But what are the most important ways in which someone can improve their credit score?\\n\\nAssistant: Here are the most important factors, from most to least important:\\n\\n   (1)  Payment history - pay on time every time,\\n   (2)  Credit utilization ratio - your total debts divided by your credit limit (suggests how much credit you’re using),\\n   (3)  Credit age - the length of time for which you’ve been responsible with your debt,\\n   (4)  Public records - anything that could appear on your public record like a bankruptcy or lien,\\n   (5)  Open accounts - any open credit cards,\\n   (6)  Recently opened accounts - the number and amount of recent credit accounts you’ve opened (and how much debt you have on those accounts),\\n   (7)  Homeownership - the amount of equity you have in your home,\\n   (8)  Length of credit history - the length of time you’ve had credit (suggests how responsible you are with credit),\\n   (9)\\n\\nHuman: And what are the most important of these in order to improve my score the most?\\n\\nAssistant:', 'original_output': ' The most important factors in order of their effect on your credit score are:\\n\\n   (1)  Payment history - pay on time every time,\\n   (2)  Credit utilization ratio - your total debts divided by your credit limit (suggests how much credit you’re using),\\n   (3)  Credit age - the length of time for which you’ve been responsible with your debt,\\n   (4)  Public records - anything that could appear on your public record like a bankruptcy or lien,\\n   (5)  Recently opened accounts - the number and amount of recent credit accounts you’ve opened (and how much debt you have on those accounts),\\n   (6)  Open accounts - any open credit cards,\\n   (7)  Homeownership - the amount of equity you have in your home,\\n   (8)  Length of credit history - the length of time you’ve had credit (suggests how responsible you are with credit),'}, {'prompt': '\\n\\nHuman: Explain how potatoes grow\\n\\nAssistant:', 'original_output': ' Potatoes grow underground, and then they send up stems called sprouts.  Those sprouts grow into the familiar potatoes we eat.'}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">7</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>prompts = [{<span style=\"color: #808000; text-decoration-color: #808000\">\"prompt\"</span>: x[<span style=\"color: #808000; text-decoration-color: #808000\">\"prompt\"</span>], <span style=\"color: #808000; text-decoration-color: #808000\">\"original_output\"</span>: x[<span style=\"color: #808000; text-decoration-color: #808000\">\"chosen\"</span>]} <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> x <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> dataset[<span style=\"color: #808000; text-decoration-color: #808000\">\"tra</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(prompts[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">10</span>])                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>eval_prompts = [{<span style=\"color: #808000; text-decoration-color: #808000\">\"prompt\"</span>: x[<span style=\"color: #808000; text-decoration-color: #808000\">\"prompt\"</span>], <span style=\"color: #808000; text-decoration-color: #808000\">\"original_output\"</span>: x[<span style=\"color: #808000; text-decoration-color: #808000\">\"chosen\"</span>]} <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> x <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> islice(    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 7 reward_fn = create_reward_fn()                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 # trlx.train(</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 #     prompts=prompts,</span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'create_reward_fn'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m7\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0mprompts = [{\u001b[33m\"\u001b[0m\u001b[33mprompt\u001b[0m\u001b[33m\"\u001b[0m: x[\u001b[33m\"\u001b[0m\u001b[33mprompt\u001b[0m\u001b[33m\"\u001b[0m], \u001b[33m\"\u001b[0m\u001b[33moriginal_output\u001b[0m\u001b[33m\"\u001b[0m: x[\u001b[33m\"\u001b[0m\u001b[33mchosen\u001b[0m\u001b[33m\"\u001b[0m]} \u001b[94mfor\u001b[0m x \u001b[95min\u001b[0m dataset[\u001b[33m\"\u001b[0m\u001b[33mtra\u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 5 \u001b[0m\u001b[96mprint\u001b[0m(prompts[\u001b[94m0\u001b[0m:\u001b[94m10\u001b[0m])                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0meval_prompts = [{\u001b[33m\"\u001b[0m\u001b[33mprompt\u001b[0m\u001b[33m\"\u001b[0m: x[\u001b[33m\"\u001b[0m\u001b[33mprompt\u001b[0m\u001b[33m\"\u001b[0m], \u001b[33m\"\u001b[0m\u001b[33moriginal_output\u001b[0m\u001b[33m\"\u001b[0m: x[\u001b[33m\"\u001b[0m\u001b[33mchosen\u001b[0m\u001b[33m\"\u001b[0m]} \u001b[94mfor\u001b[0m x \u001b[95min\u001b[0m islice(    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 7 reward_fn = create_reward_fn()                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 9 \u001b[0m\u001b[2m# trlx.train(\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0m\u001b[2m#     prompts=prompts,\u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'create_reward_fn'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# config = TRLConfig.update(default_config, hparams)\n",
    "\n",
    "dataset = load_dataset(\"Dahoas/rm-static\")\n",
    "prompts = [{\"prompt\": x[\"prompt\"], \"original_output\": x[\"chosen\"]} for x in dataset[\"train\"]]\n",
    "print(prompts[0:10])\n",
    "eval_prompts = [{\"prompt\": x[\"prompt\"], \"original_output\": x[\"chosen\"]} for x in islice(dataset[\"test\"], 280)]\n",
    "reward_fn = create_reward_fn()\n",
    "\n",
    "# trlx.train(\n",
    "#     prompts=prompts,\n",
    "#     eval_prompts=eval_prompts,\n",
    "#     reward_fn=reward_fn,\n",
    "#     config=config,\n",
    "#     stop_sequences=[\"Human:\", \"human:\", \"Assistant:\", \"assistant:\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 reward_fn(prompts[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>], prompts[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>][<span style=\"color: #808000; text-decoration-color: #808000\">'prompt'</span>], prompts[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>][<span style=\"color: #808000; text-decoration-color: #808000\">'original_output'</span>])             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 # type(prompts[0])</span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span>list indices must be integers or slices, not str\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 reward_fn(prompts[\u001b[94m0\u001b[0m:\u001b[94m1\u001b[0m], prompts[\u001b[94m0\u001b[0m:\u001b[94m1\u001b[0m][\u001b[33m'\u001b[0m\u001b[33mprompt\u001b[0m\u001b[33m'\u001b[0m], prompts[\u001b[94m0\u001b[0m:\u001b[94m1\u001b[0m][\u001b[33m'\u001b[0m\u001b[33moriginal_output\u001b[0m\u001b[33m'\u001b[0m])             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m\u001b[2m# type(prompts[0])\u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0mlist indices must be integers or slices, not str\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reward_fn(prompts[0:1], prompts[0:1][\"prompt\"], prompts[0:1][\"original_output\"])\n",
    "# type(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thw/miniconda3/envs/rlenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-15 16:03:36,472] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">32</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 29 </span>)                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 30 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 31 </span>default_config = TRLConfig(                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 32 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>train=TrainConfig(                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 33 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>seq_length=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1024</span>,                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 34 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>epochs=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">10000</span>,                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 35 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>total_steps=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">10000</span>,                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TrainConfig.__init__</span><span style=\"font-weight: bold\">()</span> missing <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> required positional argument: <span style=\"color: #008000; text-decoration-color: #008000\">'max_history_size'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m32\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 29 \u001b[0m)                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 30 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 31 \u001b[0mdefault_config = TRLConfig(                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 32 \u001b[2m│   \u001b[0mtrain=TrainConfig(                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 33 \u001b[0m\u001b[2m│   │   \u001b[0mseq_length=\u001b[94m1024\u001b[0m,                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 34 \u001b[0m\u001b[2m│   │   \u001b[0mepochs=\u001b[94m10000\u001b[0m,                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 35 \u001b[0m\u001b[2m│   │   \u001b[0mtotal_steps=\u001b[94m10000\u001b[0m,                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0m\u001b[1;35mTrainConfig.__init__\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m missing \u001b[1;36m1\u001b[0m required positional argument: \u001b[32m'max_history_size'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from itertools import islice\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import tritonclient.grpc as client_util\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "from torch import nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tritonclient.utils import np_to_triton_dtype\n",
    "\n",
    "import trlx\n",
    "from trlx.data.default_configs import (\n",
    "    ModelConfig,\n",
    "    OptimizerConfig,\n",
    "    SACConfig,\n",
    "    SchedulerConfig,\n",
    "    TokenizerConfig,\n",
    "    TrainConfig,\n",
    "    TRLConfig,\n",
    ")\n",
    "from trlx.trainer.accelerate_sac_trainer import AccelerateSACTrainer\n",
    "from trlx.utils.modeling import (\n",
    "    freeze_bottom_causal_layers,\n",
    ")\n",
    "\n",
    "default_config = TRLConfig(\n",
    "    train=TrainConfig(\n",
    "        seq_length=1024,\n",
    "        epochs=10000,\n",
    "        total_steps=10000,\n",
    "        batch_size=4,\n",
    "        eval_batch_size=32,\n",
    "        max_history_size=128,\n",
    "        checkpoint_interval=10000,\n",
    "        eval_interval=500,\n",
    "        pipeline=\"PromptPipeline\",\n",
    "        trainer=\"AccelerateSACTrainer\",\n",
    "        checkpoint_dir=\"checkpoints/sac_hh\",\n",
    "    ),\n",
    "    model=ModelConfig(model_path=\"EleutherAI/gpt-j-6B\", num_layers_unfrozen=2),\n",
    "    tokenizer=TokenizerConfig(tokenizer_path=\"EleutherAI/gpt-j-6B\", truncation_side=\"left\"),\n",
    "    optimizer=OptimizerConfig(name=\"adamw\", kwargs=dict(lr=8e-6, betas=(0.9, 0.95), eps=1.0e-8, weight_decay=1.0e-6)),\n",
    "    scheduler=SchedulerConfig(name=\"cosine_annealing\", kwargs=dict(T_max=10000, eta_min=8e-6)),\n",
    "    method=SACConfig(\n",
    "        name=\"SACConfig\",\n",
    "        num_rollouts=64,\n",
    "        # chunk_size=16,\n",
    "        chunk_size=4,\n",
    "        sac_epochs=4,\n",
    "        init_kl_coef=0.05,\n",
    "        target=6,\n",
    "        horizon=10000,\n",
    "        alpha=1,\n",
    "        beta=1,\n",
    "        gamma=0.001,\n",
    "        lam=0.95,\n",
    "        actor_reg_coef=0.9,\n",
    "        # cliprange=0.2,\n",
    "        # cliprange_value=0.2,\n",
    "        # vf_coef=1,\n",
    "        scale_reward=\"running\",\n",
    "        ref_mean=None,\n",
    "        ref_std=None,\n",
    "        cliprange_reward=10,\n",
    "        gen_kwargs=dict(\n",
    "            max_new_tokens=128,\n",
    "            top_k=0,\n",
    "            top_p=1.0,\n",
    "            do_sample=True,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "default_config.train.batch_size = 8\n",
    "default_config.train.total_steps = 1500\n",
    "default_config.train.checkpoint_dir = \"checkpoints/ppo_hh_125M\"\n",
    "default_config.model.model_path = \"Dahoas/pythia-125M-static-sft\"\n",
    "default_config.tokenizer.tokenizer_path = \"EleutherAI/gpt-neox-20b\"\n",
    "default_config.method.num_rollouts = 128\n",
    "default_config.train.tracker = None\n",
    "\n",
    "\n",
    "config = TRLConfig.update(default_config, {})\n",
    "\n",
    "# dataset = load_dataset(\"Dahoas/rm-static\")\n",
    "# prompts = [{\"prompt\": x[\"prompt\"], \"original_output\": x[\"chosen\"]} for x in dataset[\"train\"]]\n",
    "# eval_prompts = [{\"prompt\": x[\"prompt\"], \"original_output\": x[\"chosen\"]} for x in islice(dataset[\"test\"], 280)]\n",
    "# reward_fn = create_reward_fn()\n",
    "\n",
    "acceleratesactrainer = AccelerateSACTrainer(config)\n",
    "model = acceleratesactrainer.get_arch(config)\n",
    "freeze_bottom_causal_layers(model.base_model, config.model.num_layers_unfrozen)\n",
    "print(model.eval())\n",
    "\n",
    "# trlx.train(\n",
    "#     prompts=prompts,\n",
    "#     eval_prompts=eval_prompts,\n",
    "#     reward_fn=reward_fn,\n",
    "#     config=config,\n",
    "#     stop_sequences=[\"Human:\", \"human:\", \"Assistant:\", \"assistant:\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_neox.embed_in.weight True\n",
      "gpt_neox.layers.0.input_layernorm.weight False\n",
      "gpt_neox.layers.0.input_layernorm.bias False\n",
      "gpt_neox.layers.0.post_attention_layernorm.weight False\n",
      "gpt_neox.layers.0.post_attention_layernorm.bias False\n",
      "gpt_neox.layers.0.attention.query_key_value.weight False\n",
      "gpt_neox.layers.0.attention.query_key_value.bias False\n",
      "gpt_neox.layers.0.attention.dense.weight False\n",
      "gpt_neox.layers.0.attention.dense.bias False\n",
      "gpt_neox.layers.0.mlp.dense_h_to_4h.weight False\n",
      "gpt_neox.layers.0.mlp.dense_h_to_4h.bias False\n",
      "gpt_neox.layers.0.mlp.dense_4h_to_h.weight False\n",
      "gpt_neox.layers.0.mlp.dense_4h_to_h.bias False\n",
      "gpt_neox.layers.1.input_layernorm.weight False\n",
      "gpt_neox.layers.1.input_layernorm.bias False\n",
      "gpt_neox.layers.1.post_attention_layernorm.weight False\n",
      "gpt_neox.layers.1.post_attention_layernorm.bias False\n",
      "gpt_neox.layers.1.attention.query_key_value.weight False\n",
      "gpt_neox.layers.1.attention.query_key_value.bias False\n",
      "gpt_neox.layers.1.attention.dense.weight False\n",
      "gpt_neox.layers.1.attention.dense.bias False\n",
      "gpt_neox.layers.1.mlp.dense_h_to_4h.weight False\n",
      "gpt_neox.layers.1.mlp.dense_h_to_4h.bias False\n",
      "gpt_neox.layers.1.mlp.dense_4h_to_h.weight False\n",
      "gpt_neox.layers.1.mlp.dense_4h_to_h.bias False\n",
      "gpt_neox.layers.2.input_layernorm.weight False\n",
      "gpt_neox.layers.2.input_layernorm.bias False\n",
      "gpt_neox.layers.2.post_attention_layernorm.weight False\n",
      "gpt_neox.layers.2.post_attention_layernorm.bias False\n",
      "gpt_neox.layers.2.attention.query_key_value.weight False\n",
      "gpt_neox.layers.2.attention.query_key_value.bias False\n",
      "gpt_neox.layers.2.attention.dense.weight False\n",
      "gpt_neox.layers.2.attention.dense.bias False\n",
      "gpt_neox.layers.2.mlp.dense_h_to_4h.weight False\n",
      "gpt_neox.layers.2.mlp.dense_h_to_4h.bias False\n",
      "gpt_neox.layers.2.mlp.dense_4h_to_h.weight False\n",
      "gpt_neox.layers.2.mlp.dense_4h_to_h.bias False\n",
      "gpt_neox.layers.3.input_layernorm.weight False\n",
      "gpt_neox.layers.3.input_layernorm.bias False\n",
      "gpt_neox.layers.3.post_attention_layernorm.weight False\n",
      "gpt_neox.layers.3.post_attention_layernorm.bias False\n",
      "gpt_neox.layers.3.attention.query_key_value.weight False\n",
      "gpt_neox.layers.3.attention.query_key_value.bias False\n",
      "gpt_neox.layers.3.attention.dense.weight False\n",
      "gpt_neox.layers.3.attention.dense.bias False\n",
      "gpt_neox.layers.3.mlp.dense_h_to_4h.weight False\n",
      "gpt_neox.layers.3.mlp.dense_h_to_4h.bias False\n",
      "gpt_neox.layers.3.mlp.dense_4h_to_h.weight False\n",
      "gpt_neox.layers.3.mlp.dense_4h_to_h.bias False\n",
      "gpt_neox.layers.4.input_layernorm.weight False\n",
      "gpt_neox.layers.4.input_layernorm.bias False\n",
      "gpt_neox.layers.4.post_attention_layernorm.weight False\n",
      "gpt_neox.layers.4.post_attention_layernorm.bias False\n",
      "gpt_neox.layers.4.attention.query_key_value.weight False\n",
      "gpt_neox.layers.4.attention.query_key_value.bias False\n",
      "gpt_neox.layers.4.attention.dense.weight False\n",
      "gpt_neox.layers.4.attention.dense.bias False\n",
      "gpt_neox.layers.4.mlp.dense_h_to_4h.weight False\n",
      "gpt_neox.layers.4.mlp.dense_h_to_4h.bias False\n",
      "gpt_neox.layers.4.mlp.dense_4h_to_h.weight False\n",
      "gpt_neox.layers.4.mlp.dense_4h_to_h.bias False\n",
      "gpt_neox.layers.5.input_layernorm.weight False\n",
      "gpt_neox.layers.5.input_layernorm.bias False\n",
      "gpt_neox.layers.5.post_attention_layernorm.weight False\n",
      "gpt_neox.layers.5.post_attention_layernorm.bias False\n",
      "gpt_neox.layers.5.attention.query_key_value.weight False\n",
      "gpt_neox.layers.5.attention.query_key_value.bias False\n",
      "gpt_neox.layers.5.attention.dense.weight False\n",
      "gpt_neox.layers.5.attention.dense.bias False\n",
      "gpt_neox.layers.5.mlp.dense_h_to_4h.weight False\n",
      "gpt_neox.layers.5.mlp.dense_h_to_4h.bias False\n",
      "gpt_neox.layers.5.mlp.dense_4h_to_h.weight False\n",
      "gpt_neox.layers.5.mlp.dense_4h_to_h.bias False\n",
      "gpt_neox.layers.6.input_layernorm.weight False\n",
      "gpt_neox.layers.6.input_layernorm.bias False\n",
      "gpt_neox.layers.6.post_attention_layernorm.weight False\n",
      "gpt_neox.layers.6.post_attention_layernorm.bias False\n",
      "gpt_neox.layers.6.attention.query_key_value.weight False\n",
      "gpt_neox.layers.6.attention.query_key_value.bias False\n",
      "gpt_neox.layers.6.attention.dense.weight False\n",
      "gpt_neox.layers.6.attention.dense.bias False\n",
      "gpt_neox.layers.6.mlp.dense_h_to_4h.weight False\n",
      "gpt_neox.layers.6.mlp.dense_h_to_4h.bias False\n",
      "gpt_neox.layers.6.mlp.dense_4h_to_h.weight False\n",
      "gpt_neox.layers.6.mlp.dense_4h_to_h.bias False\n",
      "gpt_neox.layers.7.input_layernorm.weight False\n",
      "gpt_neox.layers.7.input_layernorm.bias False\n",
      "gpt_neox.layers.7.post_attention_layernorm.weight False\n",
      "gpt_neox.layers.7.post_attention_layernorm.bias False\n",
      "gpt_neox.layers.7.attention.query_key_value.weight False\n",
      "gpt_neox.layers.7.attention.query_key_value.bias False\n",
      "gpt_neox.layers.7.attention.dense.weight False\n",
      "gpt_neox.layers.7.attention.dense.bias False\n",
      "gpt_neox.layers.7.mlp.dense_h_to_4h.weight False\n",
      "gpt_neox.layers.7.mlp.dense_h_to_4h.bias False\n",
      "gpt_neox.layers.7.mlp.dense_4h_to_h.weight False\n",
      "gpt_neox.layers.7.mlp.dense_4h_to_h.bias False\n",
      "gpt_neox.layers.8.input_layernorm.weight False\n",
      "gpt_neox.layers.8.input_layernorm.bias False\n",
      "gpt_neox.layers.8.post_attention_layernorm.weight False\n",
      "gpt_neox.layers.8.post_attention_layernorm.bias False\n",
      "gpt_neox.layers.8.attention.query_key_value.weight False\n",
      "gpt_neox.layers.8.attention.query_key_value.bias False\n",
      "gpt_neox.layers.8.attention.dense.weight False\n",
      "gpt_neox.layers.8.attention.dense.bias False\n",
      "gpt_neox.layers.8.mlp.dense_h_to_4h.weight False\n",
      "gpt_neox.layers.8.mlp.dense_h_to_4h.bias False\n",
      "gpt_neox.layers.8.mlp.dense_4h_to_h.weight False\n",
      "gpt_neox.layers.8.mlp.dense_4h_to_h.bias False\n",
      "gpt_neox.layers.9.input_layernorm.weight False\n",
      "gpt_neox.layers.9.input_layernorm.bias False\n",
      "gpt_neox.layers.9.post_attention_layernorm.weight False\n",
      "gpt_neox.layers.9.post_attention_layernorm.bias False\n",
      "gpt_neox.layers.9.attention.query_key_value.weight False\n",
      "gpt_neox.layers.9.attention.query_key_value.bias False\n",
      "gpt_neox.layers.9.attention.dense.weight False\n",
      "gpt_neox.layers.9.attention.dense.bias False\n",
      "gpt_neox.layers.9.mlp.dense_h_to_4h.weight False\n",
      "gpt_neox.layers.9.mlp.dense_h_to_4h.bias False\n",
      "gpt_neox.layers.9.mlp.dense_4h_to_h.weight False\n",
      "gpt_neox.layers.9.mlp.dense_4h_to_h.bias False\n",
      "gpt_neox.layers.10.input_layernorm.weight True\n",
      "gpt_neox.layers.10.input_layernorm.bias True\n",
      "gpt_neox.layers.10.post_attention_layernorm.weight True\n",
      "gpt_neox.layers.10.post_attention_layernorm.bias True\n",
      "gpt_neox.layers.10.attention.query_key_value.weight True\n",
      "gpt_neox.layers.10.attention.query_key_value.bias True\n",
      "gpt_neox.layers.10.attention.dense.weight True\n",
      "gpt_neox.layers.10.attention.dense.bias True\n",
      "gpt_neox.layers.10.mlp.dense_h_to_4h.weight True\n",
      "gpt_neox.layers.10.mlp.dense_h_to_4h.bias True\n",
      "gpt_neox.layers.10.mlp.dense_4h_to_h.weight True\n",
      "gpt_neox.layers.10.mlp.dense_4h_to_h.bias True\n",
      "gpt_neox.layers.11.input_layernorm.weight True\n",
      "gpt_neox.layers.11.input_layernorm.bias True\n",
      "gpt_neox.layers.11.post_attention_layernorm.weight True\n",
      "gpt_neox.layers.11.post_attention_layernorm.bias True\n",
      "gpt_neox.layers.11.attention.query_key_value.weight True\n",
      "gpt_neox.layers.11.attention.query_key_value.bias True\n",
      "gpt_neox.layers.11.attention.dense.weight True\n",
      "gpt_neox.layers.11.attention.dense.bias True\n",
      "gpt_neox.layers.11.mlp.dense_h_to_4h.weight True\n",
      "gpt_neox.layers.11.mlp.dense_h_to_4h.bias True\n",
      "gpt_neox.layers.11.mlp.dense_4h_to_h.weight True\n",
      "gpt_neox.layers.11.mlp.dense_4h_to_h.bias True\n",
      "gpt_neox.final_layer_norm.weight True\n",
      "gpt_neox.final_layer_norm.bias True\n",
      "embed_out.weight True\n"
     ]
    }
   ],
   "source": [
    "for name, v in model.base_model.named_parameters():\n",
    "    print(name, v.requires_grad)\n",
    "# print(model.frozen_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.randint(0, 10, size=[8, 10])\n",
    "attention_mask = torch.randn(8, 10)\n",
    "outputs = model(tokens, attention_mask, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values', 'hidden_states', 'value'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight True\n",
      "0.bias True\n",
      "2.weight True\n",
      "2.bias True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.v_head.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
